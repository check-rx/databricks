# Databricks Lakehouse â€“ PBP Data Pipeline

Used for **Databricks resources**.  
Will hold **data warehousing, lakehouse, data lake, ETL**, and other scripts related to working with them within the Databricks ecosystem.  

âš ï¸ **Note**: This repo could be replaced by another environment if Databricks is not used.  
All **data cleaning** and **ETL scripts** are portable and runnable in plain **Python/Spark**.

---

# Lakehouse PBP Data Pipeline

A production-ready **Bronze â†’ Silver â†’ (Gold soon)** lakehouse for **PBP plan data**.  

### ğŸ¯ Design Principles
- ğŸ”„ **Rerunnable** â€“ idempotent overwrites with schema evolution
- ğŸ“ **Audited** â€“ every load and validation logged
- âœ… **Validated** â€“ SQL unit tests for downstream quality
- ğŸ“¦ **Portable** â€“ runnable inside Databricks *and* via `spark-submit` / `spark-sql`

---

## ğŸ“š Contents

- [Architecture](#architecture)
- [Data Flow](#data-flow)
- [Repository Layout](#repository-layout)
- [Prerequisites](#prerequisites)
- [Environment & Schemas](#environment--schemas)
- [Audit Tables](#audit-tables)
- [Run the Pipeline](#run-the-pipeline)
- [Configuration Knobs](#configuration-knobs)
- [Validation](#validation)
- [Operational Monitoring](#operational-monitoring)
- [Unit Tests (CI/CD)](#unit-tests-optional-cicd)
- [Troubleshooting](#troubleshooting--gotchas)
- [Roadmap](#roadmap-gold--rag)

---

## ğŸ§© Architecture

**Bronze (Raw)**  
- Raw copy of tab-delimited files from Unity Catalog Volumes.  
- Delta tables in `auto.bronze`.  
- No enrichment, no renames, NULLs preserved.  

**Silver (Enriched)**  
- Reads Bronze tables + joins **Benefits Dictionary**.  
- Adds context with configurable prefix (default: `bendict_`).  
- Normalizes values (`trim/upper/lpad`).  
- Outputs **long format records** `(bid_id, column_name)`.

**Gold (Planned)**  
- BI & RAG-ready.  
- `plan_facts_long` for **RAG/RIG embeddings**.  
- `plan_master` wide for **dashboards & APIs**.

---

## ğŸ”„ Data Flow

```text
/Volumes/auto/landing/plans/*.txt
   â”‚
   â–¼
auto.bronze.<file>   (Bronze ingestion)
   â”‚
   â–¼
auto.silver.<file>_enriched   (Silver enrichment w/ dictionary)
   â”‚
   â–¼
auto.gold.*   (coming soon)


lakehouse-pbp/
â”œâ”€â”€ README.md
â”œâ”€â”€ audit/
â”‚   â””â”€â”€ audit_schema.sql        # CREATE TABLEs for audit logs
â”œâ”€â”€ bronze/
â”‚   â””â”€â”€ bronze_ingest.py        # Bronze loader
â”œâ”€â”€ silver/
â”‚   â””â”€â”€ silver_enrich.py        # Silver enricher
â”œâ”€â”€ validation/
â”‚   â””â”€â”€ silver_validation.sql   # SQL unit tests
â”œâ”€â”€ run_pipeline.py             # Orchestrator (bronze â†’ silver â†’ validation)
â””â”€â”€ tests/
    â”œâ”€â”€ test_bronze.py
    â””â”€â”€ test_silver.py



âš™ï¸ Prerequisites

Databricks workspace with Unity Catalog enabled

Catalog: auto (default, configurable)

Input: /Volumes/auto/landing/plans/*.txt

Tab-delimited (\t)

With headers (else adjust scripts)

Benefits dictionary in auto.landing.pbp_dictionary

Columns: file, name, title, field_title, json_question, codes, code_values


ğŸ—ï¸ Environment & Schemas
CREATE SCHEMA IF NOT EXISTS auto.bronze;
CREATE SCHEMA IF NOT EXISTS auto.silver;
CREATE SCHEMA IF NOT EXISTS auto.audit;


ğŸ“œ Audit Tables
-- Load log
CREATE TABLE IF NOT EXISTS auto.audit.load_log (
  timestamp     TIMESTAMP,
  step          STRING,
  source_file   STRING,
  output_table  STRING,
  action        STRING,
  status        STRING,
  row_count     BIGINT,
  error_message STRING
);

-- Validation log
CREATE TABLE IF NOT EXISTS auto.audit.validation_log (
  timestamp   TIMESTAMP,
  layer       STRING,
  table_name  STRING,
  test_name   STRING,
  status      STRING,
  details     STRING
);


â–¶ï¸ Run the Pipeline
ğŸŸ¢ Databricks (Interactive)

Run bronze/bronze_ingest.py â†’ writes to auto.bronze.*

Run silver/silver_enrich.py â†’ writes to auto.silver.*_enriched

Run validation/silver_validation.sql â†’ logs to auto.audit.validation_log

ğŸ”µ Databricks Workflow

Job with 3 tasks:

bronze_ingest.py

silver_enrich.py

silver_validation.sql


ğŸŸ  Outside Databricks (Spark CLI)
spark-submit bronze/bronze_ingest.py
spark-submit silver/silver_enrich.py
spark-sql -f validation/silver_validation.sql


Or run the orchestrator:
python run_pipeline.py



